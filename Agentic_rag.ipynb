{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installing dependencies"
      ],
      "metadata": {
        "id": "lyeBq0rhjcMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community langchain-core langgraph chromadb langchain-tavily langchain-groq"
      ],
      "metadata": {
        "id": "5sLijvuVLVMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install docx2txt pypdf unstructured dotenv"
      ],
      "metadata": {
        "id": "Y6jXMftiNbgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting structures"
      ],
      "metadata": {
        "id": "7uS5FfRcj0c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import BaseMessage\n",
        "from typing import List, Literal\n",
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class RouteDecision(BaseModel):\n",
        "    route: Literal[\"rag\", \"answer\", \"end\"]\n",
        "    reply: str | None = Field(None, description=\"Filled only when route == 'end'\")\n",
        "\n",
        "class RagJudge(BaseModel):\n",
        "    sufficient: bool\n",
        "\n",
        "class AgentState(TypedDict, total=False):\n",
        "    messages: List[BaseMessage]\n",
        "    route:    Literal[\"rag\", \"answer\", \"end\"]\n",
        "    rag:      str\n",
        "    web:      str\n",
        "\n",
        "class State(TypedDict):\n",
        "    \"\"\"State for our chatbot - this holds the conversation history\"\"\"\n",
        "    # The add_messages function handles appending new messages to the conversation\n",
        "    messages: Annotated[list[BaseMessage], add_messages]"
      ],
      "metadata": {
        "id": "TTt1nQvCK17c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Nodes and edges"
      ],
      "metadata": {
        "id": "wEMe7vEEj9Ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_5XE84tKhS1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_groq import ChatGroq\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "router_llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\\\n",
        "             .with_structured_output(RouteDecision)\n",
        "\n",
        "def from_router(st: AgentState) -> Literal[\"rag\", \"answer\", \"end\"]:\n",
        "    return st[\"route\"]\n",
        "\n",
        "def after_rag(st: AgentState) -> Literal[\"answer\", \"web\"]:\n",
        "    return st[\"route\"]\n",
        "\n",
        "def after_web(_) -> Literal[\"answer\"]:\n",
        "    return \"answer\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "def rag_node(state: AgentState) -> AgentState:\n",
        "    print(\"RAG node invoked\")\n",
        "    query = next((m.content for m in reversed(state[\"messages\"])\n",
        "                  if isinstance(m, HumanMessage)), \"\")\n",
        "\n",
        "    chunks = rag_search_tool.invoke({\"query\": query})\n",
        "    # Use structured output to judge if RAG results are sufficient\n",
        "    judge_messages = [\n",
        "        (\"system\", (\n",
        "            \"You are a judge evaluating if the retrieved information is sufficient \"\n",
        "            \"to answer the user's question. Consider both relevance and completeness.\"\n",
        "        )),\n",
        "        (\"user\", f\"Question: {query}\\n\\nRetrieved info: {chunks}\\n\\nIs this sufficient to answer the question?\")\n",
        "    ]\n",
        "    verdict: RagJudge = judge_llm.invoke(judge_messages)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"rag\": chunks,\n",
        "        \"route\": \"answer\" if verdict.sufficient else \"web\"\n",
        "    }\n",
        "\n",
        "def web_node(state: AgentState) -> AgentState:\n",
        "    print(\"Web node invoked\")\n",
        "    query = next((m.content for m in reversed(state[\"messages\"])\n",
        "                  if isinstance(m, HumanMessage)), \"\")\n",
        "    snippets = web_search_tool.invoke({\"query\": query})\n",
        "    return {**state, \"web\": snippets, \"route\": \"answer\"}\n",
        "\n",
        "def chatbot_node(state: AgentState) -> AgentState:\n",
        "    print(\"Chatbot node invoked\")\n",
        "    \"\"\"\n",
        "    The chatbot node that decides whether to use tools or provide direct responses\n",
        "    \"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "\n",
        "    # Add system prompt\n",
        "    system_message = \"\"\"You are a helpful AI assistant. You have access to web search and calculator tools.\n",
        "\n",
        "    Use the web_search tool when:\n",
        "    - Asked about current events, news, or recent information\n",
        "    - Need to find specific facts or data\n",
        "    - Asked about real-time information (weather, stock prices, etc.)\n",
        "\n",
        "    Use the calculator tool when:\n",
        "    - Asked to perform mathematical calculations\n",
        "    - Need to solve math problems\n",
        "\n",
        "    For general knowledge questions that don't require real-time data, answer directly.\n",
        "    Be helpful and conversational in your responses.\"\"\"\n",
        "\n",
        "    # Prepare messages with system prompt\n",
        "    all_messages = [{\"role\": \"system\", \"content\": system_message}] + messages\n",
        "\n",
        "    # Get response from LLM\n",
        "    response = llm_with_tools.invoke(all_messages)\n",
        "\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def should_continue(state: AgentState) -> Literal[\"tools\", \"end\"]:\n",
        "    \"\"\"\n",
        "    Determine whether to continue to tools or end the conversation\n",
        "    \"\"\"\n",
        "    print(\"Judge node invoked\")\n",
        "    messages = state.get(\"messages\", [])\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    # If the last message has tool calls, go to tools\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    # Otherwise, end the conversation\n",
        "    else:\n",
        "        return \"end\"\n",
        "\n",
        "def answer_node(state: AgentState) -> AgentState:\n",
        "    print(\"Answer node invoked\")\n",
        "    user_q = next((m.content for m in reversed(state.get(\"messages\", []))\n",
        "                   if isinstance(m, HumanMessage)), \"\")\n",
        "\n",
        "    ctx_parts = []\n",
        "    if state.get(\"rag\"):\n",
        "        ctx_parts.append(\"Knowledge Base Information:\\n\" + state[\"rag\"])\n",
        "    if state.get(\"web\"):\n",
        "        ctx_parts.append(\"Web Search Results:\\n\" + state[\"web\"])\n",
        "\n",
        "    context = \"\\n\\n\".join(ctx_parts) if ctx_parts else \"No external context available.\"\n",
        "\n",
        "    prompt = f\"\"\"Please answer the user's question using the provided context.\n",
        "\n",
        "Question: {user_q}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Provide a helpful, accurate, and concise response based on the available information.\"\"\"\n",
        "\n",
        "    ans = answer_llm.invoke([HumanMessage(content=prompt)]).content\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=ans)]\n",
        "    }\n",
        "\n",
        "def router_node(state: AgentState) -> AgentState:\n",
        "    query = next((m.content for m in reversed(state[\"messages\"])\n",
        "                  if isinstance(m, HumanMessage)), \"\")\n",
        "\n",
        "    # Use structured output properly - pass messages directly\n",
        "    messages = [\n",
        "        (\"system\", (\n",
        "            \"You are a router that decides how to handle user queries:\\n\"\n",
        "            \"- Use 'end' for pure greetings/small-talk (also provide a 'reply')\\n\"\n",
        "            \"- Use 'rag' when knowledge base lookup is needed\\n\"\n",
        "            \"- Use 'answer' when you can answer directly without external info\"\n",
        "        )),\n",
        "        (\"user\", query)\n",
        "    ]\n",
        "\n",
        "    result: RouteDecision = router_llm.invoke(messages)\n",
        "\n",
        "    out = {\"messages\": state[\"messages\"], \"route\": result.route}\n",
        "    if result.route == \"end\":\n",
        "        out[\"messages\"] = state[\"messages\"] + [AIMessage(content=result.reply or \"Hello!\")]\n",
        "    return out"
      ],
      "metadata": {
        "id": "tjF0V1DIK4zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#For uploading files"
      ],
      "metadata": {
        "id": "u_fby6mikRi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredHTMLLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import chromadb\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "import os\n",
        "\n",
        "# Configure text splitter and persistent Chroma client\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, length_function=len)\n",
        "client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "vectorstore = client.get_or_create_collection(\"documents\")\n",
        "\n",
        "\n",
        "def load_and_split_document(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a document from disk and split it into chunks.\n",
        "    Supported formats: PDF, DOCX, HTML.\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.pdf'):\n",
        "        loader = PyPDFLoader(file_path)\n",
        "    elif file_path.endswith('.docx'):\n",
        "        loader = Docx2txtLoader(file_path)\n",
        "    elif file_path.endswith('.html'):\n",
        "        loader = UnstructuredHTMLLoader(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
        "\n",
        "    documents = loader.load()\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "def index_document_to_chroma(file_path: str, file_id: int) -> bool:\n",
        "    \"\"\"\n",
        "    Index a document by splitting into chunks, assigning unique IDs,\n",
        "    and storing metadata for each chunk for easy deletion later.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        splits = load_and_split_document(file_path)\n",
        "        print(f\"working with file {file_id}\")\n",
        "\n",
        "        # Prepare data for Chroma\n",
        "        documents = [split.page_content for split in splits]\n",
        "        metadatas = []\n",
        "        ids = []\n",
        "        for i, split in enumerate(splits):\n",
        "            # Unique chunk ID: combine file_id with chunk index\n",
        "            unique_id = f\"{file_id}_{i}\"\n",
        "            ids.append(unique_id)\n",
        "            # Attach file_id and optional source path to metadata\n",
        "            md = split.metadata.copy()\n",
        "            md['file_id'] = str(file_id)\n",
        "            md['source'] = os.path.basename(file_path)\n",
        "            metadatas.append(md)\n",
        "\n",
        "        # Upsert chunks into the collection\n",
        "        vectorstore.upsert(\n",
        "            documents=documents,\n",
        "            ids=ids,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "        #vectorstore.persist()\n",
        "        print(f\"file {file_id} uploaded\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error indexing document: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_relevant_documents(query, k=2):\n",
        "    results = vectorstore.query(\n",
        "        query_texts=[query],\n",
        "        n_results=k\n",
        "    )\n",
        "    return results['documents'][0][0]+\"\\n\\n\"+results['documents'][0][1]\n",
        "\n",
        "# index_document_to_chroma('/content/Company_ GreenFields BioTech.docx',1)\n",
        "# index_document_to_chroma('/content/Company_ QuantumNext Systems.docx',2)\n",
        "# index_document_to_chroma('/content/Company_ TechWave Innovations.docx',3)\n",
        "# index_document_to_chroma('/content/GreenGrow Innovations_ Company History.docx',4)\n",
        "# index_document_to_chroma(\"/content/GreenGrow's EcoHarvest System_ A Revolution in Farming.pdf\",5)"
      ],
      "metadata": {
        "id": "Vm1DVKY9LJZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tools"
      ],
      "metadata": {
        "id": "iP263cTUkd20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "tavily = TavilySearch(max_results=3, topic=\"general\")\n",
        "@tool\n",
        "def web_search_tool(query: str) -> str:\n",
        "    \"\"\"Up-to-date web info via Tavily\"\"\"\n",
        "    print(\"Web search tool invoked\")\n",
        "    try:\n",
        "        result = tavily.invoke({\"query\": query})\n",
        "\n",
        "        # Extract and format the results from Tavily response\n",
        "        if isinstance(result, dict) and 'results' in result:\n",
        "            formatted_results = []\n",
        "            for item in result['results']:\n",
        "                title = item.get('title', 'No title')\n",
        "                content = item.get('content', 'No content')\n",
        "                url = item.get('url', '')\n",
        "                formatted_results.append(f\"Title: {title}\\nContent: {content}\\nURL: {url}\")\n",
        "\n",
        "            return \"\\n\\n\".join(formatted_results) if formatted_results else \"No results found\"\n",
        "        else:\n",
        "            return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"WEB_ERROR::{e}\"\n",
        "\n",
        "@tool\n",
        "def rag_search_tool(query: str) -> str:\n",
        "    \"\"\"Top-3 chunks from KB (empty string if none)\"\"\"\n",
        "    print(\"RAG search tool invoked\")\n",
        "    try:\n",
        "        return get_relevant_documents(query, k=2)\n",
        "    except Exception as e:\n",
        "        return f\"RAG_ERROR::{e}\"\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Calculate mathematical expressions. Use this for any math calculations.\"\"\"\n",
        "    print(\"Calculator tool invoked\")\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return f\"The result of {expression} is {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating {expression}: {str(e)}\""
      ],
      "metadata": {
        "id": "SzjwECLxKuvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building graph"
      ],
      "metadata": {
        "id": "jIZNrPvSkvlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def router_node(state: AgentState) -> AgentState:\n",
        "    query = next((m.content for m in reversed(state[\"messages\"])\n",
        "                  if isinstance(m, HumanMessage)), \"\")\n",
        "\n",
        "    # Use structured output properly - pass messages directly\n",
        "    messages = [\n",
        "        (\"system\", (\n",
        "            \"You are a router that decides how to handle user queries:\\n\"\n",
        "            \"- Use 'end' for pure greetings/small-talk (also provide a 'reply')\\n\"\n",
        "            \"- Use 'rag' when knowledge base lookup is needed\\n\"\n",
        "            \"- Use 'answer' when you can answer directly without external info\"\n",
        "        )),\n",
        "        (\"user\", query)\n",
        "    ]\n",
        "\n",
        "    result: RouteDecision = router_llm.invoke(messages)\n",
        "\n",
        "    out = {\"messages\": state[\"messages\"], \"route\": result.route}\n",
        "    if result.route == \"end\":\n",
        "        out[\"messages\"] = state[\"messages\"] + [AIMessage(content=result.reply or \"Hello!\")]\n",
        "    return out\n",
        "\n",
        "tools = [calculator, web_search_tool]\n",
        "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0.7)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "judge_llm  = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\\\n",
        "             .with_structured_output(RagJudge)\n",
        "answer_llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0.7)\n",
        "router_llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\\\n",
        "             .with_structured_output(RouteDecision)\n",
        "\n",
        "g = StateGraph(AgentState)\n",
        "g.add_node(\"router\", router_node)\n",
        "g.add_node(\"rag_lookup\", rag_node)\n",
        "g.add_node(\"web_search\", web_node)\n",
        "g.add_node(\"answer\", answer_node)\n",
        "\n",
        "g.set_entry_point(\"router\")\n",
        "g.add_conditional_edges(\"router\", from_router,\n",
        "                        {\"rag\": \"rag_lookup\", \"answer\": \"answer\", \"end\": END})\n",
        "g.add_conditional_edges(\"rag_lookup\", after_rag,\n",
        "                        {\"answer\": \"answer\", \"web\": \"web_search\"})\n",
        "g.add_edge(\"web_search\",  \"answer\")\n",
        "g.add_edge(\"answer\", END)\n",
        "\n",
        "agent = g.compile(checkpointer=MemorySaver())"
      ],
      "metadata": {
        "id": "Hy6Iuc27LNi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#For final run"
      ],
      "metadata": {
        "id": "m74Kvsfikyy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    config = {\"configurable\": {\"thread_id\": \"thread-12\"}}\n",
        "    print(\"RAG Agent CLI (type 'quit' or 'exit' to stop)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nYou: \").strip()\n",
        "        if q.lower() in {\"quit\", \"exit\"}:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            result = agent.invoke(\n",
        "                {\"messages\": [HumanMessage(content=q)]},\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            # Get the last AI message\n",
        "            last_message = next((m for m in reversed(result[\"messages\"])\n",
        "                               if isinstance(m, AIMessage)), None)\n",
        "\n",
        "            if last_message:\n",
        "                print(f\"Agent: {last_message.content}\")\n",
        "            else:\n",
        "                print(\"Agent: No response generated\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\nGoodbye!\")"
      ],
      "metadata": {
        "id": "R0n8AKj2kses"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}